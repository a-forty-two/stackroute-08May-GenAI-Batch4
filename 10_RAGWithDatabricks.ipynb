{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8585c1d-69e2-49ac-86d0-00eddc9d9e9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install pypdf databricks-vectorsearch\n",
    "%pip install sentence-transformers\n",
    "dbutils.library.restartPython()  # Restart Python to apply changes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87893b09-8b4c-4d10-ab2e-b45e67bda537",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from pypdf import PdfReader\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from databricks.vector_search.client import VectorSearchClient\n",
    "from transformers import AutoModel, AutoTokenizer, pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "651e0284-1e35-4187-8c67-9954ca63da76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pdf_path = \"/Workspace/Users/shantanu@meteoros.in/Mantis_shrimp_facts.pdf\" #path to your Mantis Shrimp facts PDF\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    reader = PdfReader(pdf_path)\n",
    "    text = \"\\n\".join([page.extract_text() for page in reader.pages if page.extract_text()])\n",
    "    return text\n",
    "\n",
    "document_text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "print(document_text[:1000])  # Show first 1000 characters for verification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5d6ca6a-e4a7-412a-8e2a-6ad79053ae7d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def chunk_text(text, chunk_size=512):\n",
    "    sentences = text.split(\". \")\n",
    "    chunks = []\n",
    "    chunk = \"\"\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        if len(chunk) + len(sentence) < chunk_size:\n",
    "            chunk += sentence + \". \"\n",
    "        else:\n",
    "            chunks.append(chunk.strip())\n",
    "            chunk = sentence + \". \"\n",
    "    \n",
    "    if chunk:\n",
    "        chunks.append(chunk.strip())\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# Apply chunking\n",
    "chunks = chunk_text(document_text, chunk_size=512)\n",
    "\n",
    "# Display chunk information\n",
    "print(f\"Total Chunks: {len(chunks)}\")\n",
    "print(\"Sample Chunk:\\n\", chunks[0])\n",
    "print()\n",
    "print(\"Sample Chunk:\\n\", chunks[1])\n",
    "print()\n",
    "print(\"Sample Chunk:\\n\", chunks[2])\n",
    "print()\n",
    "print(\"Sample Chunk:\\n\", chunks[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd7edf72-2333-40f3-b2a2-3fdef15bc5be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Only to be uncommented if first chunk is empty. Otherwise comment it. \n",
    "chunks = chunks[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc5c9b36-ecce-4c04-aa3a-27904622192d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convert chunks to Spark DataFrame\n",
    "spark.sql(\"USE hive_metastore\")  # Ensure we use the default catalog\n",
    "\n",
    "df = spark.createDataFrame([(i, chunk) for i, chunk in enumerate(chunks)], [\"id\", \"text\"])\n",
    "\n",
    "# Save to Delta Table in hive_metastore\n",
    "df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"rag_chunks\")\n",
    "\n",
    "print(\"Document chunks stored in Delta Table (hive_metastore.rag_chunks)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba1c94d5-bc6a-40ea-ac87-44b5c37c58cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Now let's write the data into a delta live table.\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"RAG\").getOrCreate()\n",
    "\n",
    "# Convert chunks to Spark DataFrame\n",
    "df = spark.createDataFrame([(i, chunk) for i, chunk in enumerate(chunks)], [\"id\", \"text\"])\n",
    "\n",
    "# Define Delta table path\n",
    "delta_table_path = \"/mnt/rag_delta_table\"\n",
    "\n",
    "# Save as Delta Table\n",
    "df.write.format(\"delta\").mode(\"overwrite\").save(delta_table_path)\n",
    "print(\"Text chunks stored in Delta Table.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "adb3a1a7-c9b1-4a77-9e92-a4b721f4b205",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load Hugging Face embedding model\n",
    "embedding_model = \"BAAI/bge-large-en\"\n",
    "model = SentenceTransformer(embedding_model)\n",
    "\n",
    "def generate_embedding(text):\n",
    "    return model.encode(text).tolist()\n",
    "\n",
    "# Generate embeddings for each chunk\n",
    "embeddings = [(i, generate_embedding(chunk)) for i, chunk in enumerate(chunks)]\n",
    "\n",
    "# Convert to Spark DataFrame\n",
    "embeddings_df = spark.createDataFrame(embeddings, [\"id\", \"embedding\"])\n",
    "\n",
    "# Save embeddings to Delta Table in hive_metastore\n",
    "embeddings_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"rag_embeddings\")\n",
    "\n",
    "print(\"Embeddings stored in Delta Table (hive_metastore.rag_embeddings)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6116e73-f81f-484f-9fe0-056beadf55c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "import numpy as np\n",
    "\n",
    "# Function to compute cosine similarity\n",
    "def cosine_similarity(a, b):\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "# Define a query embedding\n",
    "query_text = \"What does mantis shrimp eat?\"\n",
    "query_embedding = model.encode(query_text).tolist()\n",
    "\n",
    "# Load embeddings from Delta Table\n",
    "df_embeddings = spark.sql(\"SELECT * FROM rag_embeddings\").toPandas()\n",
    "\n",
    "# Compute similarity scores\n",
    "df_embeddings[\"similarity\"] = df_embeddings[\"embedding\"].apply(lambda x: cosine_similarity(query_embedding, x))\n",
    "\n",
    "# Get top 3 most relevant chunks\n",
    "top_chunks = df_embeddings.sort_values(by=\"similarity\", ascending=False).head(3)\n",
    "\n",
    "# Retrieve corresponding text chunks\n",
    "retrieved_chunks = spark.sql(\"SELECT * FROM rag_chunks\").toPandas()\n",
    "retrieved_texts = retrieved_chunks[retrieved_chunks[\"id\"].isin(top_chunks[\"id\"])][\"text\"].tolist()\n",
    "\n",
    "print(\"ðŸ”Ž Retrieved Chunks:\")\n",
    "for text in retrieved_texts:\n",
    "    print('******')\n",
    "    print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aad44dd9-3562-48e7-b467-b2b2e2febba0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load Hugging Face LLM \n",
    "llm_pipeline = pipeline(\"text-generation\", model=\"facebook/opt-1.3b\")\n",
    "# very small model-> facebook/opt-6.7b is better but may crash the kernel \n",
    "\n",
    "# Generate response based on retrieved context\n",
    "context = \"\\n\".join(retrieved_texts)\n",
    "prompt = f\"Context: {context}\\n\\nQuestion: {query_text}\\nAnswer:\"\n",
    "\n",
    "response = llm_pipeline(prompt, max_length=150, num_return_sequences=1)\n",
    "print(\"ðŸ’¡ AI Response:\\n\", response[0][\"generated_text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51ca96d3-d9fb-4e26-8214-0179179f3b90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Generate response based on retrieved context\n",
    "context = \"\\n\".join(retrieved_texts)\n",
    "prompt = f\"Context: {context}\\n\\nQuestion: {query_text}\\nAnswer:\"\n",
    "\n",
    "response = llm_pipeline(prompt, max_length=150, num_return_sequences=1)\n",
    "print(\"ðŸ’¡ AI Response:\\n\", response[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad5af9bd-dd0d-4692-979d-22240cd50e6a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Below code is for high level understanding and WILL NOT EXECUTE. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "884ac474-c09a-428a-bafc-82682b866ae0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# WARNING: FOLLOWING CODE WILL NOT WORK AND THROW ERROR DUE TO UNITY CATALOG/PERMISSIONS \n",
    "\n",
    "# Part 1- building embeddings and vectors\n",
    "\n",
    "import mlflow\n",
    "from databricks.vector_search.client import VectorSearchClient\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Initialize Databricks Vector Search Client\n",
    "vsc = VectorSearchClient()\n",
    "\n",
    "# Define Unity Catalog schema and table\n",
    "catalog_name = \"your_catalog\"\n",
    "schema_name = \"your_schema\"\n",
    "vector_index_name = \"your_vector_index\"\n",
    "\n",
    "# Load Databricks Model for Embeddings\n",
    "embedding_model_uri = \"models:/your_embedding_model/production\"\n",
    "embedding_model = mlflow.pyfunc.load_model(embedding_model_uri)\n",
    "\n",
    "def generate_embeddings(texts):\n",
    "    \"\"\"Generate embeddings using Databricks MLflow Model.\"\"\"\n",
    "    return [embedding_model.predict([text])[0] for text in texts]\n",
    "\n",
    "# Create or load the Vector Search index\n",
    "try:\n",
    "    vsc.get_index(catalog_name, schema_name, vector_index_name)\n",
    "except Exception:\n",
    "    vsc.create_index(catalog_name, schema_name, vector_index_name, dimension=768)\n",
    "\n",
    "# Load dataset from Unity Catalog\n",
    "df = spark.read.table(f\"{catalog_name}.{schema_name}.your_text_table\")\n",
    "\n",
    "# Generate embeddings and store them in Vector Search\n",
    "text_data = df.select(\"text_column\").rdd.map(lambda row: row[0]).collect()\n",
    "embeddings = generate_embeddings(text_data)\n",
    "\n",
    "# Convert to DataFrame and store in Vector Search\n",
    "vector_df = spark.createDataFrame(\n",
    "    [(i, text, embedding) for i, (text, embedding) in enumerate(zip(text_data, embeddings))],\n",
    "    [\"id\", \"text\", \"embedding\"]\n",
    ")\n",
    "\n",
    "vector_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{catalog_name}.{schema_name}.{vector_index_name}\")\n",
    "\n",
    "print(\"Embeddings stored in Unity Catalog and Databricks Vector Search.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "036a94ef-e987-4904-ae07-6073dcf91777",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# WARNING: FOLLOWING CODE WILL NOT WORK AND THROW ERROR DUE TO UNITY CATALOG/PERMISSIONS\n",
    "\n",
    "# Part 2- RAG \n",
    "\n",
    "# Load a Databricks-hosted LLM model for RAG\n",
    "llm_model_uri = \"models:/your_llm_model/production\"\n",
    "llm_model = mlflow.pyfunc.load_model(llm_model_uri)\n",
    "\n",
    "def retrieve_relevant_chunks(query, top_k=5):\n",
    "    \"\"\"Retrieve relevant text chunks using Databricks Vector Search.\"\"\"\n",
    "    query_embedding = embedding_model.predict([query])[0]\n",
    "    results = vsc.search(\n",
    "        catalog_name, schema_name, vector_index_name,\n",
    "        query_embedding, top_k=top_k\n",
    "    )\n",
    "    return [row[\"text\"] for row in results]\n",
    "\n",
    "def generate_response(query):\n",
    "    \"\"\"Generate response using retrieved chunks and LLM.\"\"\"\n",
    "    relevant_chunks = retrieve_relevant_chunks(query)\n",
    "    context = \"\\n\".join(relevant_chunks)\n",
    "    prompt = f\"Context:\\n{context}\\n\\nQuestion: {query}\\n\\nAnswer:\"\n",
    "    response = llm_model.predict([prompt])[0]\n",
    "    return response\n",
    "\n",
    "# Example usage\n",
    "query = \"What is a Mantis Shrimp?\"\n",
    "response = generate_response(query)\n",
    "print(\"Generated Response:\", response)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "10_RAGWithDatabricks",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}